{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HRL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool Release Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRANetwork(nn.Module):\n",
    "    def __init__(self, input_dim=5, embedding_dim=128, hidden_dims=[256, 128], output_dim=2):\n",
    "        super(PRANetwork, self).__init__()\n",
    "        self.feature_extractor = nn.Linear(input_dim, embedding_dim)\n",
    "        layers = []\n",
    "        for i in range(len(hidden_dims)):\n",
    "            in_dim = embedding_dim if i == 0 else hidden_dims[i-1]\n",
    "            out_dim = hidden_dims[i]\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.ff_network = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.ff_network(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route Planning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_layers, order_dim=7, crowdsource_dim=5, inhouse_dim=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.order_embedding = nn.Linear(order_dim, dim)                # order_dim = 7\n",
    "        self.crowdsource_embedding = nn.Linear(crowdsource_dim, dim)    # crowdsource_dim = 5\n",
    "        self.inhouse_embedding = nn.Linear(inhouse_dim, dim)            # inhouse_dim = 3\n",
    "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, num_heads, batch_first=True) for _ in range(num_layers)])\n",
    "        self.norm_layers = nn.ModuleList([nn.BatchNorm1d(dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x_o, x_c, x_i):\n",
    "        h_o = self.order_embedding(x_o)\n",
    "        h_c = self.crowdsource_embedding(x_c)\n",
    "        h_i = self.inhouse_embedding(x_i)\n",
    "        \n",
    "        x = torch.cat((h_o, h_c, h_i), dim=0)\n",
    "        \n",
    "        for multi_head_attn, norm in zip(self.layers, self.norm_layers):\n",
    "            attn_output, _ = multi_head_attn(x, x, x)\n",
    "            x = x + attn_output\n",
    "            x = norm(x)\n",
    "            x = x + F.relu(x)\n",
    "            x = norm(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim, output_dim, total_nodes, S, num_heads, is_cdrpa=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.multi_head_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.cdrpa_embedding = nn.Linear(dim*2+1+1, dim)\n",
    "        self.cvrpa_embedding = nn.Linear(dim*2+1, dim)\n",
    "        self.is_cdrpa = is_cdrpa\n",
    "        self.W_Qs = nn.Linear(dim, dim)\n",
    "        self.W_Ks = nn.Linear(dim, dim)\n",
    "        self.output_layer = nn.Linear(total_nodes, output_dim)\n",
    "        self.S = S\n",
    "\n",
    "    def forward(self, context, H, mask):\n",
    "        if self.is_cdrpa:\n",
    "            context = self.cdrpa_embedding(context)\n",
    "        else: \n",
    "            context = self.cvrpa_embedding(context)\n",
    "            \n",
    "        H_c_t, _ = self.multi_head_attn(context, H, H)\n",
    "        Qs = self.W_Qs(H_c_t)\n",
    "        Ks = self.W_Ks(H)\n",
    "\n",
    "        u = self.S * torch.tanh((Qs @ Ks.transpose(0, 1)) / torch.sqrt(torch.tensor(Qs.size(-1), dtype=torch.float32)))\n",
    "        u = u.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        output = self.output_layer(u)\n",
    "        \n",
    "        return F.softmax(output, dim=1)\n",
    "    \n",
    "class RoutePolicyNetwork(nn.Module):\n",
    "    def __init__(self, dim, S, num_heads, num_layers):\n",
    "        super(RoutePolicyNetwork, self).__init__()\n",
    "        self.encoder = Encoder(dim, num_heads, num_layers)\n",
    "        \n",
    "    def forward(self, P_o, P_c, P_i, game1_result):\n",
    "        P_c = sorted(P_c, key=lambda x: x['l_c'])           # Sorting crowdsource with latest depature time l_c\n",
    "        \n",
    "        x_o = torch.tensor([[*o['v_o'], *o['d_o'], o['e_o'], o['l_o'], o['w_o']] for o in P_o], dtype=torch.float32)\n",
    "        x_c = torch.tensor([[*c['u_c'], c['a_c'], c['l_c'], c['q_c']] for c in P_c], dtype=torch.float32)\n",
    "        x_i = torch.tensor([[*i['u_i'], i['q_i']] for i in P_i], dtype=torch.float32)\n",
    "        \n",
    "        H_N = self.encoder(x_o, x_c, x_i)                   # Embedding graph -> forward pass\n",
    "        \n",
    "        h_o = H_N[:x_o.shape[0], :].clone()\n",
    "        h_c = H_N[x_o.shape[0]:x_o.shape[0]+x_c.shape[0], :].clone()\n",
    "        h_i = H_N[x_o.shape[0]+x_c.shape[0]:, :].clone()\n",
    "        \n",
    "        assert h_o.shape[0] == x_o.shape[0], 'Invalid shape'\n",
    "        assert h_c.shape[0] == x_c.shape[0], 'Invalid shape'\n",
    "        assert h_i.shape[0] == x_i.shape[0], 'Invalid shape'\n",
    "        \n",
    "        order_indices = list(game1_result.keys())\n",
    "        P_o_c = [P_o[idx] for idx in order_indices]\n",
    "        \n",
    "        x_o_c = torch.stack([x_o[idx] for idx in order_indices])\n",
    "        H_N_c = self.encoder(x_o_c, x_c, x_i)\n",
    "        h_o_c = H_N[:x_o_c.shape[0], :].clone()\n",
    "        h_N_c_mean = torch.mean(H_N_c, dim=0)\n",
    "        \n",
    "        h_N_mean = torch.mean(H_N, dim=0)     # Global graph embedding\n",
    "        \n",
    "        # print(H_N.shape)\n",
    "        # print(h_N_mean.shape)\n",
    "        # print(f\"Total Orders: {x_o.size(0)}\")\n",
    "        # print(f\"Total Crowdsources: {x_c.size(0)}\")\n",
    "        # print(f\"Total Inhouses: {x_i.size(0)}\")\n",
    "        \n",
    "        # output_dim must be the number of the order nodes\n",
    "        decoder_cdrpa = Decoder(dim=128, output_dim=x_o_c.size(0), S=1, total_nodes=H_N_c.size(0), num_heads=8, is_cdrpa=True)\n",
    "        decoder_cvrpa = Decoder(dim=128, output_dim=x_o.size(0), S=1, total_nodes=H_N.size(0), num_heads=8)\n",
    "\n",
    "        S_CDPRA_t = copy.deepcopy(P_o)                  # State S_CDPRA_t = P_o -> node of orders\n",
    "        # print(f\"P_o nodes: {S_CDPRA_t}\")\n",
    "        \n",
    "        mask = torch.zeros(H_N.size(0)).bool()          # Mask for the nodes\n",
    "        visited_idx_node_Po = []                        # Visited index of node in P_o\n",
    "        routes_policy_c = []                            # Routes policy for c\n",
    "        node_probs = []                                 # Probability of the selected nodes\n",
    "        log_probs = []                                  # Log probabilities of the selected nodes\n",
    "        for idx_c in range(x_c.size(0)):                # CDRPA\n",
    "            if len(visited_idx_node_Po) >= len(P_o_c): \n",
    "                break\n",
    "            \n",
    "            t = 1\n",
    "            q = 0\n",
    "            t_travel = 0                                # Total time travel\n",
    "            h_uc = h_c[idx_c]                           # t = 1, the driver’s location information is the destination’s embedded feature h_uc\n",
    "            c = P_c[idx_c]                              # Crowdsource's information\n",
    "            Q = c['q_c']                                # Limitation capacity of crowdsource\n",
    "            L = c['l_c'] - c['a_c']                     # Limitation total time travel (driver's point -> pick-up -> next pick-up)\n",
    "            H_c_t_prev = None                           # Previous embedding previous visited node \n",
    "            v_o_prev = []                               # Previous v_o node\n",
    "            route = []\n",
    "            visited_idx_node_c = []                     # Visited node index for c\n",
    "            q_res = 0\n",
    "            t_travel_res = 0\n",
    "            while q < Q and t_travel < L:\n",
    "                q_r = torch.tensor(Q - q, dtype=torch.float32).unsqueeze(0)\n",
    "                t_travel_r = torch.tensor(L - t_travel, dtype=torch.float32).unsqueeze(0)\n",
    "                H_c_t = torch.cat((h_N_c_mean, h_uc, q_r, t_travel_r), dim=-1) if t == 1 else torch.cat((h_N_c_mean, H_c_t_prev, q_r, t_travel_r), dim=-1)\n",
    "                \n",
    "                probs_node = decoder_cdrpa(H_c_t.unsqueeze(0), H_N_c, mask)   # Probability distribution over node of Orders\n",
    "                \n",
    "                node = torch.multinomial(probs_node, 1)                     # Order node selection using sampling method\n",
    "                p_node = probs_node.squeeze(0)[node.item()]\n",
    "                log_prob = torch.log(p_node)                                # Log probability of the selected node\n",
    "                \n",
    "                o = P_o_c[node.item()]                                        # Next visited Order node\n",
    "                \n",
    "                if (q + o['w_o']) > Q:                                \n",
    "                    q_res = q                                               # Over-capacity\n",
    "                            \n",
    "                q += o['w_o']                                               # Consider capacity\n",
    "                \n",
    "                if t == 1:\n",
    "                    t_travel += self.t_uo(c['u_c'], o['v_o']) + self.t_uo(o['v_o'], o['d_o'])\n",
    "                    t_travel_res += t_travel\n",
    "                elif (t_travel + self.t_uo(v_o_prev, o['v_o'])) > L:\n",
    "                    t_travel_res = t_travel                                                     # Over-time travel\n",
    "                    t_travel += self.t_uo(v_o_prev, o['v_o'])\n",
    "                else:\n",
    "                    t_travel += self.t_uo(v_o_prev, o['v_o']) + self.t_uo(o['v_o'], o['d_o'])   # Time travel from prev v_o to next v_o, v_o to d_o\n",
    "                \n",
    "                route += [o] if q < Q and t_travel < L else []\n",
    "                visited_idx_node_c += [node.item()] if q < Q and t_travel < L else []\n",
    "                log_probs += [log_prob.item()] if q < Q and t_travel < L else []\n",
    "                node_probs += [p_node.item()] if q < Q and t_travel < L else []\n",
    "                \n",
    "                if node.item() not in visited_idx_node_Po and q < Q and t_travel < L:                  \n",
    "                    visited_idx_node_Po.append(node.item())                                     # Keep track visited node\n",
    "                \n",
    "                v_o_prev = o['d_o']\n",
    "                H_c_t_prev = h_o[node.item()]                                                   # Embedded feature of the driver’s previous visited node\n",
    "                    \n",
    "                t += 1\n",
    "            \n",
    "            # print(\"--------------------------------------------\")\n",
    "            # print(f\"Driver {c}\")\n",
    "            # print(f\"Total capacity: {q_res}\")\n",
    "            # print(f\"Total travel time: {t_travel_res}\")\n",
    "            # print(f\"Route: {route}\")\n",
    "            # print(f\"Visited idx node c: {visited_idx_node_c}\")\n",
    "            # print(f\"Visited P_o: {visited_idx_node_Po}\")\n",
    "            # print(f\"Visited len(P_o): {len(visited_idx_node_Po)}\")\n",
    "            routes_policy_c.append({\n",
    "                \"crowdsource\": c,\n",
    "                \"total_capacity\": q_res,\n",
    "                \"total_travel_time\": t_travel_res,\n",
    "                \"route\": route,\n",
    "                \"visited_idx_node\": visited_idx_node_c,\n",
    "            })\n",
    "        \n",
    "        routes_policy_i = []                    # Routes policy for i\n",
    "        for idx_i in range(x_i.size(0)):        # CVRPA\n",
    "            if len(visited_idx_node_Po) >= len(P_o): \n",
    "                break\n",
    "            \n",
    "            t = 1\n",
    "            q = 0\n",
    "            h_ui = h_i[idx_i]                   # t = 1, the driver’s location information is the destination’s embedded feature h_uc\n",
    "            i = P_i[idx_i]                      # Crowdsource's information\n",
    "            Q = i['q_i']                        # Limitation capacity of inhouse delivery\n",
    "            H_k_t_prev = None                   # Previous embedding previous visited node \n",
    "            route = []\n",
    "            visited_idx_node_i = []             # Visited node index for c\n",
    "            q_res = 0\n",
    "            while q < Q:\n",
    "                q_r = torch.tensor(Q - q, dtype=torch.float32).unsqueeze(0)\n",
    "                H_k_t = torch.cat((h_N_mean, h_ui, q_r), dim=-1) if t == 1 else torch.cat((h_N_mean, H_k_t_prev, q_r), dim=-1)\n",
    "                \n",
    "                probs_node = decoder_cvrpa(H_k_t.unsqueeze(0), H_N, mask)       # Probability distribution over node of orders\n",
    "                \n",
    "                node = torch.multinomial(probs_node, 1)                         # Order node selection using sampling method\n",
    "                p_node = probs_node.squeeze(0)[node.item()]\n",
    "                log_prob = torch.log(p_node)                                    # Logarit probability of the selected node\n",
    "                \n",
    "                o = P_o[node.item()]                                            # Next visited Order node\n",
    "                if (q + o['w_o']) > Q:                                \n",
    "                    q_res = q                                                   # Over-capacity\n",
    "                            \n",
    "                q += o['w_o']                                                   # Consider capacity\n",
    "                \n",
    "                route += [o] if q < Q else []\n",
    "                visited_idx_node_i += [node.item()] if q < Q else []\n",
    "                log_probs += [log_prob.item()] if q < Q else []\n",
    "                node_probs += [p_node.item()] if q < Q else []\n",
    "                \n",
    "                if node.item() not in visited_idx_node_Po and q < Q:                      # Keep track visited node\n",
    "                    visited_idx_node_Po.append(node.item())\n",
    "                    \n",
    "                H_k_t_prev = h_o[node.item()]                                   # Embedded feature of the driver’s previous visited node\n",
    "                    \n",
    "                t += 1\n",
    "            \n",
    "            # print(\"--------------------------------------------\")\n",
    "            # print(f\"In-house delivery {i}\")\n",
    "            # print(f\"Total capacity: {q_res}\")\n",
    "            # print(f\"Route: {route}\")\n",
    "            # print(f\"Visited idx node i: {visited_idx_node_i}\")\n",
    "            # print(f\"Visited P_o: {visited_idx_node_Po}\")\n",
    "            # print(f\"Visited len(P_o): {len(visited_idx_node_Po)}\")\n",
    "            \n",
    "            routes_policy_i.append({\n",
    "                \"inhouse\": i,\n",
    "                \"total_capacity\": q_res,\n",
    "                \"route\": route,\n",
    "                \"visited_idx_node\": visited_idx_node_i,\n",
    "            })\n",
    "\n",
    "            \n",
    "        return routes_policy_c, routes_policy_i, log_probs, node_probs\n",
    "    \n",
    "    def t_uo(self, u_c, v_o):\n",
    "        speed = 30\n",
    "        return np.around(np.sqrt((u_c[0] - v_o[0])**2 + (u_c[1] - v_o[1])**2) / speed, 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_strategy_player1 = [\"Offer\", \"Not_Offer\"]\n",
    "list_strategy_player2 = [\"Accept\", \"Reject\"]\n",
    "\n",
    "prob_action1 = np.array([0.15, 0.85])\n",
    "prob_action2 = np.array([0.55, 0.45])\n",
    "\n",
    "class NashGame:\n",
    "\tdef __init__(self, P_o_tau, P_c_tau, crowdsources_cost, motobike_cost, crowdsources_self_cost):\n",
    "\t\tself.crowdsources_cost = crowdsources_cost\n",
    "\t\tself.motobike_cost = motobike_cost\n",
    "\t\tself.crowdsources_self_cost = crowdsources_self_cost\n",
    "\n",
    "\tdef t_oc(self, u_c, v_o):\n",
    "\t\tspeed = 30\n",
    "\t\treturn np.around(np.sqrt((u_c[0] - v_o[0])**2 + (u_c[1] - v_o[1])**2) / speed, 2)\n",
    "\n",
    "\tdef find_NE(self, P_o_tau, P_c_tau):\n",
    "\t\tpayoff_player1 = np.zeros((len(P_o_tau), len(P_c_tau), len(list_strategy_player1), len(list_strategy_player2)))\n",
    "\t\tpayoff_player2 = np.zeros((len(P_o_tau), len(P_c_tau), len(list_strategy_player1), len(list_strategy_player2)))\n",
    "  \n",
    "\t\tbest_player1_value = np.zeros((len(P_o_tau), len(P_c_tau)))\n",
    "\t\tbest_player2_value = np.zeros((len(P_o_tau), len(P_c_tau)))\n",
    "\t\tpayoff_player1_NNE = np.zeros((len(P_o_tau), len(P_c_tau), len(list_strategy_player1), len(list_strategy_player2)))\n",
    "\t\tpayoff_player2_NNE = np.zeros((len(P_o_tau), len(P_c_tau), len(list_strategy_player1), len(list_strategy_player2)))\n",
    "\t\tbest_player1_indices, best_player2_indices = {}, {}\n",
    "\t\tNE_value, NE_action = {}, {}\n",
    "\t\tNNE_action, random_NNE, NNE_list = {}, {}, {}\n",
    "\t\tsummation, min_summation = {}, {}\n",
    "\t\tgame1_result = {}, {}, {}\n",
    "  \n",
    "\t\tfor idx_o, o in enumerate(P_o_tau):\n",
    "\t\t\tbreak_inner_loop = False\n",
    "\t\t\tfor idx_c, c in enumerate(P_c_tau):\n",
    "\t\t\t\tcompensation = self.t_oc(o['v_o'], o['d_o']) * self.crowdsources_cost\n",
    "\t\t\t\tdistance_cost = self.t_oc(o['v_o'], o['d_o']) * self.motobike_cost\n",
    "\t\t\t\tdistance_crs = self.t_oc(c['u_c'], o['v_o']) + self.t_oc(o['v_o'], o['d_o'])\n",
    "\t\t\t\tcrs_cost_total = self.crowdsources_self_cost * distance_crs\n",
    "\n",
    "\t\t\t\tpayoff_player1[idx_o, idx_c, 0, 0] = payoff_player1[idx_o, idx_c, 0, 1] = compensation\n",
    "\t\t\t\tpayoff_player1[idx_o, idx_c, 1, 0] = payoff_player1[idx_o, idx_c, 1, 1] = distance_cost\n",
    "\n",
    "\t\t\t\t# Player 2 payoffs\n",
    "\t\t\t\tpayoff_player2[idx_o, idx_c, 0, 0] = compensation - crs_cost_total\n",
    "\t\t\t\tpayoff_player2[idx_o, idx_c, 0, 1] = -compensation\n",
    "\t\t\t\tpayoff_player2[idx_o, idx_c, 1, 0] = -crs_cost_total\n",
    "\t\t\t\tpayoff_player2[idx_o, idx_c, 1, 1] = -(compensation - crs_cost_total)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tpayoff_player1_NNE = payoff_player1.copy()\n",
    "\t\t\t\tpayoff_player2_NNE = payoff_player2.copy()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Player 1 best response and index\n",
    "\t\t\t\tmin_values_across_rows = np.min(payoff_player1[idx_o, idx_c], axis=1)\n",
    "\t\t\t\tbest_player1_value[idx_o, idx_c] = np.max(min_values_across_rows)\n",
    "\t\t\t\tpayoff_player1_array = np.array(payoff_player1[(idx_o, idx_c)])\n",
    "\t\t\t\tindices1 = np.where(payoff_player1_array == best_player1_value[(idx_o, idx_c)])\n",
    "\t\t\t\tbest_player1_indices[idx_o, idx_c] = [indices1[0][0], indices1[1][0]]            \n",
    "\n",
    "\t\t\t\t# Player 2 best response and index\n",
    "\t\t\t\tmax_values_across_rows = np.max(payoff_player2[idx_o, idx_c], axis=1)\n",
    "\t\t\t\tbest_player2_value[idx_o, idx_c] = np.min(max_values_across_rows)\n",
    "\n",
    "\t\t\t\tpayoff_player2_array = np.array(payoff_player2[(idx_o, idx_c)])\n",
    "\t\t\t\tindices2 = np.where(payoff_player2_array == best_player2_value[(idx_o, idx_c)])\n",
    "\t\t\t\tbest_player2_indices[idx_o, idx_c] = [indices2[0][0], indices2[1][0]]\n",
    "\n",
    "\t\t\t\tfor p1_idx in best_player1_indices[idx_o, idx_c]:\n",
    "\t\t\t\t\tif p1_idx in best_player2_indices[idx_o, idx_c]:\n",
    "\t\t\t\t\t\tNE_value[idx_o, idx_c] = ([best_player1_value[idx_o, idx_c], best_player2_value[idx_o, idx_c]])\n",
    "\t\t\t\t\t\tNE_action[idx_o, idx_c] = best_player1_indices[idx_o, idx_c]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tpayoff_player1_NNE[idx_o, idx_c] -= best_player1_value[idx_o, idx_c]\n",
    "\t\t\t\t\t\tpayoff_player2_NNE[idx_o, idx_c] = best_player2_value[idx_o, idx_c] - payoff_player2[idx_o, idx_c]\n",
    "\t\t\t\t\t\tsummation[idx_o, idx_c] = payoff_player1_NNE[idx_o, idx_c] + payoff_player2_NNE[idx_o, idx_c]\n",
    "\t\t\t\t\t\tmin_summation[idx_o, idx_c] = np.min(summation[idx_o, idx_c])\n",
    "\t\t\t\t\t\tNNE_list[idx_o, idx_c] = [min_summation[idx_o, idx_c]]\n",
    "\t\t\t\t\t\tif len(NNE_list[idx_o, idx_c]) == 1:\n",
    "\t\t\t\t\t\t\trandom_NNE[idx_o, idx_c] = NNE_list[idx_o, idx_c][0]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\trandom_NNE[idx_o, idx_c] = random.choice(NNE_list[idx_o, idx_c])\n",
    "\t\t\t\t\t\tNNE_action[idx_o, idx_c] = np.argwhere(summation[idx_o, idx_c] == random_NNE[idx_o, idx_c])\n",
    "\t\t\t\t\t\tfor key, value in NNE_action.items():\n",
    "\t\t\t\t\t\t\tNNE_action[key] = value.flatten()\n",
    "\n",
    "\t\t\t\tif (idx_o, idx_c) in NE_action and NE_action[(idx_o, idx_c)] == [0, 0]:\n",
    "\t\t\t\t\tbreak_inner_loop = True \n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tif break_inner_loop:\n",
    "\t\t\t\tcontinue\n",
    "              \n",
    "\t\t# Classify orders used crowdsourcee\n",
    "\t\tgame1_result = {}\n",
    "\t\tfor (idx_o, idx_c), value in NE_action.items():\n",
    "\t\t\tif value == [0, 0]:\n",
    "\t\t\t\tif idx_o not in game1_result:\n",
    "\t\t\t\t\tgame1_result[idx_o] = [idx_c]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tgame1_result[idx_o].append(idx_c)\n",
    "\n",
    "\t\tfor (idx_o, idx_c), value in NNE_action.items():\n",
    "\t\t\tif np.array_equal(value, np.array([0, 0])):\n",
    "\t\t\t\tif idx_o not in game1_result:\n",
    "\t\t\t\t\tgame1_result[idx_o] = [idx_c]\n",
    "\t\t\t\telif c not in game1_result[idx_o]:  # Ensure no duplicate entries\n",
    "\t\t\t\t\tgame1_result[idx_o].append(idx_c)\n",
    "\n",
    "\t\treturn NE_action, NNE_action, game1_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_criteria = 3 #On-time Reliability, Star Reputation, Experience\n",
    "class Ranking:\n",
    "    def __init__(self, num_criteria, num_crowdsources):\n",
    "        self.num_criteria = num_criteria\n",
    "        self.num_crowdsources = num_crowdsources\n",
    "        self.fuzzy_scale = {\n",
    "            (1, 1): (1, 1, 3), (1, 2): (1, 3, 5), (1, 3): (3, 5, 7), (1, 4): (5, 7, 9),\n",
    "            (2, 1): (1/5, 1/3, 1), (3, 1): (1/7, 1/5, 1/3), (4, 1): (1/9, 1/7, 1/5)\n",
    "        }\n",
    "        self.crxcr = np.array([[[1, 1, 3], [1, 3, 5], [1, 3, 5]],\n",
    "                               [[1/5, 1/3, 1], [1, 1, 3], [1, 1, 3]],\n",
    "                               [[1/5, 1/3, 1], [1, 1, 3], [1, 1, 3]]])\n",
    "        \n",
    "        self.alt = self.generate_pairwise_matrices()\n",
    "        self.RI = {\n",
    "            1: 0.00, 2: 0.00, 3: 0.58, 4: 0.90, 5: 1.12,\n",
    "            6: 1.24, 7: 1.32, 8: 1.41, 9: 1.45, 10: 1.49\n",
    "        }\n",
    "\n",
    "    def generate_pairwise_matrices(self):\n",
    "        pair_wise_matrices = []\n",
    "        for _ in range(self.num_criteria):\n",
    "            pair_wise_matrix = np.zeros((self.num_crowdsources, self.num_crowdsources, 3))\n",
    "            for i in range(self.num_crowdsources):\n",
    "                for j in range(self.num_crowdsources):\n",
    "                    if i == j:\n",
    "                        pair_wise_matrix[i, j, :] = (1, 1, 3)\n",
    "                    else:\n",
    "                        key = random.choice(list(self.fuzzy_scale.keys()))\n",
    "                        pair_wise_matrix[i, j, :] = self.fuzzy_scale[key]\n",
    "                        l, m, u = self.fuzzy_scale[key]\n",
    "                        pair_wise_matrix[j, i, :] = (1/u, 1/m, 1/l)\n",
    "            pair_wise_matrices.append(pair_wise_matrix)\n",
    "        return np.stack(pair_wise_matrices)\n",
    "\n",
    "    def isConsistent(self, matrix):\n",
    "        mat_len = len(matrix)\n",
    "        midMatrix = np.zeros((mat_len, mat_len))\n",
    "        for i in range(mat_len):\n",
    "            for j in range(mat_len):\n",
    "                midMatrix[i][j] = matrix[i][j][1]\n",
    "                \n",
    "        eigenvalue = np.real(np.linalg.eigvals(midMatrix))\n",
    "        lambdaMax = max(eigenvalue)\n",
    "\n",
    "        RIValue = self.RI[mat_len]\n",
    "        CIValue = (lambdaMax - mat_len) / (mat_len - 1)\n",
    "        CRValue = CIValue / RIValue\n",
    "        if CRValue <= 0.1:\n",
    "            if printComp: print(\"Matrix reasonably consistent, we could continue\")\n",
    "            return True\n",
    "        else:\n",
    "            if printComp: print(\"Consistency Ratio is greater than 10%, we need to revise the subjective judgment\")\n",
    "            return False\n",
    "        \n",
    "    def pairwiseComp(self, matrix):\n",
    "        matrix_len = len(matrix)\n",
    "        geoMean = np.zeros((len(matrix), 3))\n",
    "        for i in range(matrix_len):\n",
    "            for j in range(3):\n",
    "                temp = 1\n",
    "                for tfn in matrix[i]:\n",
    "                    temp *= tfn[j]\n",
    "                temp = pow(temp, 1 / matrix_len)\n",
    "                geoMean[i, j] = temp\n",
    "\n",
    "        geoMean_sum = np.zeros(3)\n",
    "        for row in geoMean:\n",
    "            geoMean_sum[0] += row[0]\n",
    "            geoMean_sum[1] += row[1]\n",
    "            geoMean_sum[2] += row[2]\n",
    "\n",
    "        weights = np.zeros(matrix_len)\n",
    "        for i in range(len(geoMean)):\n",
    "            temp = 0\n",
    "            for j in range(len(geoMean[0])):\n",
    "                temp += geoMean[i, j] * (1 / geoMean_sum[(3 - 1) - j])\n",
    "            weights[i] = temp\n",
    "\n",
    "        normWeights = np.zeros(matrix_len)\n",
    "        weights_sum = np.sum(weights)\n",
    "        for i in range(matrix_len):\n",
    "            normWeights[i] = weights[i] / weights_sum\n",
    "\n",
    "        return normWeights\n",
    "\n",
    "    def FAHP(self):\n",
    "        crxcr_weights = self.pairwiseComp(self.crxcr)            \n",
    "        crowdsource_weights = np.zeros((len(self.alt), len(self.alt[0])))\n",
    "        for i, alt_cr in enumerate(self.alt):\n",
    "            crowdsource_weights[i] = self.pairwiseComp(alt_cr)\n",
    "            \n",
    "        crowdsource_weights = crowdsource_weights.transpose(1, 0)\n",
    "        \n",
    "        crowdsource_weight = np.zeros(len(self.alt[0]))\n",
    "        for i in range(len(self.alt[0])):\n",
    "            crowdsource_weight[i] = np.dot(crxcr_weights, crowdsource_weights[i])\n",
    "        \n",
    "        NE_action, NNE_action, game1_result = find_NE(num_orders, num_crowdsources)\n",
    "        crs_weights_by_order = {}\n",
    "        for order, crowdsource in game1_result.items():\n",
    "            crowdsource_weight = Ranking(num_criteria, len(crowdsource)).FAHP()\n",
    "            crs_weights_by_order[order] = crowdsource_weight\n",
    "\n",
    "        return crs_weights_by_order\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [0, 1], [0, 1], [1, 2], [0, 1]]\n",
      "[[6, 6], [5, 4], [4, 6], [0, 9], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "grid_size = 3\n",
    "grid_spacing = 0.25\n",
    "num_grid_points = int(grid_size / grid_spacing)\n",
    "grid_points = [[x, y] for x in range(num_grid_points) for y in range(num_grid_points)]\n",
    "random.seed(2)\n",
    "\n",
    "def generate_coordinates(num, initial=False):\n",
    "    if initial:\n",
    "        num_zero_one = int(num * 0.85)\n",
    "        num_other = num - num_zero_one\n",
    "\n",
    "        depot_coords = [[0, 1]] * num_zero_one\n",
    "\n",
    "        other_coords = random.sample(grid_points, num_other)\n",
    "\n",
    "        sample = depot_coords + other_coords\n",
    "        random.shuffle(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    return random.sample(grid_points, num)\n",
    "\n",
    "print(generate_coordinates(5, initial=True))\n",
    "print(generate_coordinates(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random orders pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'e_o': 69.05, 'l_o': 66.67, 'v_o': [0, 1], 'd_o': [6, 9], 'w_o': 5.34},\n",
       " {'e_o': 83.21, 'l_o': 73.75, 'v_o': [8, 4], 'd_o': [8, 1], 'w_o': 3.17},\n",
       " {'e_o': 64.75, 'l_o': 64.52, 'v_o': [0, 1], 'd_o': [9, 0], 'w_o': 7.09},\n",
       " {'e_o': 74.11, 'l_o': 69.2, 'v_o': [0, 1], 'd_o': [11, 2], 'w_o': 8.14},\n",
       " {'e_o': 66.49, 'l_o': 65.39, 'v_o': [0, 1], 'd_o': [3, 6], 'w_o': 1.04},\n",
       " {'e_o': 77.09, 'l_o': 70.69, 'v_o': [0, 1], 'd_o': [11, 11], 'w_o': 1.42},\n",
       " {'e_o': 65.55, 'l_o': 64.92, 'v_o': [0, 1], 'd_o': [3, 9], 'w_o': 0.87},\n",
       " {'e_o': 74.43, 'l_o': 69.36, 'v_o': [0, 1], 'd_o': [5, 0], 'w_o': 6.99},\n",
       " {'e_o': 82.79, 'l_o': 73.54, 'v_o': [0, 1], 'd_o': [4, 11], 'w_o': 9.11},\n",
       " {'e_o': 77.61, 'l_o': 70.95, 'v_o': [9, 2], 'd_o': [0, 6], 'w_o': 4.22}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_random_orders_pool(num_orders, time_step, len_time_step=10, initial=False):\n",
    "    \"\"\"\n",
    "    time_step: 0, 1, 2,..., 54\n",
    "    len_time_step: length of a period = 10\n",
    "    e_o: early delivery time\n",
    "    l_o: late delivery time\n",
    "    v_o: co-ordinates pick up order\n",
    "    d_o: co-ordinates delivery order\n",
    "    a_o: order arrived time\n",
    "    p_o: pick up time\n",
    "    w_o: weight order\n",
    "    tw_o: time window between pick up and late delivery time\n",
    "    \"\"\"\n",
    "    a_o = np.around(\n",
    "        np.around(np.random.uniform(0, len_time_step, num_orders), 2)\n",
    "        + time_step * len_time_step, 2\n",
    "    )\n",
    "    p_o = np.around(a_o + np.around(np.random.uniform(0, 45), 2), 2)\n",
    "    tw_o = np.around(np.random.uniform(60, 120), 2)\n",
    "    e_o = np.around(a_o + p_o + tw_o, 2)\n",
    "    l_o = np.around(p_o + tw_o, 2)\n",
    "    w_o = np.around(np.random.uniform(0.5, 10, num_orders), 2)\n",
    "    v_o = generate_coordinates(num_orders, initial=initial)\n",
    "    d_o = generate_coordinates(num_orders)\n",
    "\n",
    "    return [{\"e_o\": i, \"l_o\": j, \"v_o\": k, \"d_o\": n, \"w_o\": m} for i, j, k, m, n in zip(e_o, l_o, v_o, w_o, d_o)]\n",
    "\n",
    "\n",
    "pool_orders = generate_random_orders_pool(10, 0, 10, initial=True)\n",
    "pool_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_o = torch.tensor([[*o['v_o'], o['e_o'], o['l_o'], o['w_o']] for o in pool_orders], dtype=torch.long)\n",
    "x_o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random crowdsources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_crowdsources_pool(\n",
    "    num_crowdsources, time_step, len_time_step=10, initial=False\n",
    "):\n",
    "    \"\"\"\n",
    "    time_step: 0, 1, 2,..., 54\n",
    "    len_time_step: length of a period = 10\n",
    "    a_c: arrive time\n",
    "    l_c: leave time\n",
    "    q_c: carrying capacity\n",
    "    u_c: co-ordinates crowdsource\n",
    "    \"\"\"\n",
    "    a_c = np.around(\n",
    "        np.around(np.random.uniform(0, len_time_step, num_crowdsources), 2)\n",
    "        + time_step * len_time_step, 2\n",
    "    )\n",
    "    l_c = np.around(a_c + np.around(np.random.uniform(120, 180), 2), 2)\n",
    "    q_c = np.around(np.random.uniform(5, 20, num_crowdsources), 2)\n",
    "    u_c = generate_coordinates(num_crowdsources)\n",
    "\n",
    "    return [{\"a_c\": i, \"l_c\": j, \"q_c\": k, \"u_c\": m} for i, j, k, m in zip(a_c, l_c, q_c, u_c)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random in-house delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_inhouse_pool(\n",
    "    num_trucks, num_motorbikes\n",
    "):\n",
    "    return [{\"q_i\": 1000, \"u_i\": [0,1]}]*num_trucks + [{\"q_i\": 25, \"u_i\": [0,1]}]*num_motorbikes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate travel time between nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = 30\n",
    "\n",
    "def t_oc(u_c, v_o):\n",
    "    return np.around(np.sqrt((u_c[0] - v_o[0])**2 + (u_c[1] - v_o[1])**2) / speed, 2)\n",
    "\n",
    "def t_ave(P_o_tau, P_c_tau):\n",
    "    return sum(t_oc(u_c=c['u_c'], v_o=o['v_o']) for o in P_o_tau for c in P_c_tau) / len(P_c_tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(routes_policy_c,\n",
    "                    routes_policy_i,\n",
    "                    t_oc,\n",
    "                    truck_cost,\n",
    "                    motobike_cost,\n",
    "                    late_penalty,\n",
    "                    duplicate_penalty,\n",
    "                    crowdsources_cost):\n",
    "   \n",
    "    waiting_cost_c = crowdsources_cost*1.5\n",
    "    # rewards_per_route_c = []  # List to store rewards for each route\n",
    "    rewards = []\n",
    "    for index, c in enumerate(routes_policy_c):\n",
    "        route = c['route']\n",
    "        if not route:\n",
    "            # rewards_per_route_c.append([])\n",
    "            rewards.extend([])\n",
    "            continue\n",
    "\n",
    "        # route_rewards = []  # Rewards for the current route\n",
    "        T_pi_c = t_oc(c['crowdsource']['u_c'], route[0]['v_o'])  # Initial travel time\n",
    "        initial_waiting_cost = max(route[0]['e_o'] - T_pi_c, 0) * waiting_cost_c\n",
    "        T_pi_c += t_oc(route[0]['v_o'], route[0]['d_o'])\n",
    "        late_delivery_penalty = late_penalty if T_pi_c > route[0]['l_o'] else 0\n",
    "        duplicate_penalty = 0  # Check for duplicate visit\n",
    "        initial_reward = (crowdsources_cost * T_pi_c + initial_waiting_cost + late_delivery_penalty + duplicate_penalty)\n",
    "        # route_rewards.append(initial_reward)\n",
    "        rewards.append(initial_reward)\n",
    "        # return\n",
    "        for n in range(len(route) - 1):\n",
    "            # Travel from current delivery to next pickup\n",
    "            T_pi_c += t_oc(route[n]['d_o'], route[n+1]['v_o'])\n",
    "            duplicate_penalty = duplicate_penalty if c['visited_idx_node'][n+1] in c['visited_idx_node'] else 0\n",
    "            waiting_time_cost = max(route[n+1]['e_o'] - T_pi_c, 0) * waiting_cost_c\n",
    "            T_pi_c += t_oc(route[n+1]['v_o'], route[n+1]['d_o'])\n",
    "            late_delivery_penalty = late_penalty if T_pi_c > route[n+1]['l_o'] else 0\n",
    "\n",
    "            node_reward = (crowdsources_cost * T_pi_c + waiting_time_cost + late_delivery_penalty)\n",
    "            # route_rewards.append(node_reward)\n",
    "            rewards.append(node_reward)\n",
    "       \n",
    "    # Reward for route i\n",
    "    rewards_per_route_i = []  # List to store rewards for each route for in-house drivers\n",
    "\n",
    "    for index, i in enumerate(routes_policy_i):\n",
    "        route = i['route']\n",
    "        if not route:\n",
    "            # rewards_per_route_i.append([])\n",
    "            rewards_per_route_i.extend([])\n",
    "            continue\n",
    "\n",
    "        route_rewards = []  # Rewards for the current route\n",
    "        waiting_cost_i = 1.5*truck_cost if i['inhouse'] == 1000 else 1.5*motobike_cost\n",
    "        T_pi_i = t_oc(i['inhouse']['u_i'], route[0]['v_o'])\n",
    "        initial_waiting_cost = max(route[0]['e_o'] - T_pi_i, 0) * waiting_cost_i\n",
    "        T_pi_i += t_oc(route[0]['v_o'], route[0]['d_o'])\n",
    "        late_delivery_penalty = late_penalty if T_pi_i > route[0]['l_o'] else 0\n",
    "        duplicate_penalty =  0\n",
    "        inhouse_cost = 1\n",
    "        if i['inhouse'] == 1000:\n",
    "            inhouse_cost = truck_cost\n",
    "        else:\n",
    "            inhouse_cost = motobike_cost\n",
    "           \n",
    "        initial_reward = (inhouse_cost * T_pi_i + initial_waiting_cost + late_delivery_penalty + duplicate_penalty)\n",
    "        # route_rewards.append(initial_reward)\n",
    "        rewards_per_route_i.append(initial_reward)\n",
    "\n",
    "\n",
    "        for n in range(len(route) - 1):\n",
    "            T_pi_i += t_oc(route[n]['d_o'], route[n+1]['v_o'])\n",
    "            duplicate_penalty = duplicate_penalty if i['visited_idx_node'][n+1] in i['visited_idx_node'] else 0\n",
    "            waiting_time_cost = max(route[n+1]['e_o'] - T_pi_i, 0) * waiting_cost_i\n",
    "            T_pi_i += t_oc(route[n+1]['v_o'], route[n+1]['d_o'])\n",
    "            late_delivery_penalty = late_penalty if T_pi_i > route[n+1]['l_o'] else 0\n",
    "\n",
    "            node_reward = (inhouse_cost * T_pi_i + waiting_time_cost + late_delivery_penalty)\n",
    "            # route_rewards.append(node_reward)\n",
    "            rewards_per_route_i.append(node_reward)\n",
    "    print(\"TC_c\", sum(rewards))\n",
    "    print(\"TC_i\", sum(rewards_per_route_i))\n",
    "    print(\"TC\", sum(rewards)+sum(rewards_per_route_i))\n",
    "   \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward at each node selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rewards_at_each_node(routes_policy_c, \n",
    "                                   routes_policy_i, \n",
    "                                   t_oc, \n",
    "                                   truck_cost,\n",
    "                                   motobike_cost,\n",
    "                                   late_penalty, \n",
    "                                   duplicate_penalty, \n",
    "                                   crowdsources_cost):\n",
    "    \n",
    "    waiting_cost_c = crowdsources_cost*1.5\n",
    "    # rewards_per_route_c = []  # List to store rewards for each route\n",
    "    rewards = []\n",
    "    for index, c in enumerate(routes_policy_c):\n",
    "        route = c['route']\n",
    "        if not route:\n",
    "            # rewards_per_route_c.append([])\n",
    "            rewards.extend([])\n",
    "            continue\n",
    "\n",
    "        # route_rewards = []  # Rewards for the current route\n",
    "        T_pi_c = t_oc(c['crowdsource']['u_c'], route[0]['v_o'])  # Initial travel time\n",
    "        initial_waiting_cost = max(route[0]['e_o'] - T_pi_c, 0) * waiting_cost_c\n",
    "        T_pi_c += t_oc(route[0]['v_o'], route[0]['d_o'])\n",
    "        late_delivery_penalty = late_penalty if T_pi_c > route[0]['l_o'] else 0\n",
    "        duplicate_penalty = 0  # Check for duplicate visit\n",
    "        initial_reward = -(crowdsources_cost * T_pi_c + initial_waiting_cost + late_delivery_penalty + duplicate_penalty)\n",
    "        # route_rewards.append(initial_reward)\n",
    "        rewards.append(initial_reward)\n",
    "        # return\n",
    "        for n in range(len(route) - 1):\n",
    "            # Travel from current delivery to next pickup\n",
    "            T_pi_c += t_oc(route[n]['d_o'], route[n+1]['v_o'])\n",
    "            duplicate_penalty = duplicate_penalty if c['visited_idx_node'][n+1] in c['visited_idx_node'] else 0\n",
    "            waiting_time_cost = max(route[n+1]['e_o'] - T_pi_c, 0) * waiting_cost_c\n",
    "            T_pi_c += t_oc(route[n+1]['v_o'], route[n+1]['d_o'])\n",
    "            late_delivery_penalty = late_penalty if T_pi_c > route[n+1]['l_o'] else 0\n",
    "\n",
    "            node_reward = -(crowdsources_cost * T_pi_c + waiting_time_cost + late_delivery_penalty)\n",
    "            # route_rewards.append(node_reward)\n",
    "            rewards.append(node_reward)\n",
    "\n",
    "        # rewards_per_route_c.append(route_rewards)\n",
    "        \n",
    "    # Reward for route i\n",
    "    rewards_per_route_i = []  # List to store rewards for each route for in-house drivers\n",
    "\n",
    "    for index, i in enumerate(routes_policy_i):\n",
    "        route = i['route']\n",
    "        if not route:\n",
    "            # rewards_per_route_i.append([])\n",
    "            rewards.extend([])\n",
    "            continue\n",
    "\n",
    "        route_rewards = []  # Rewards for the current route\n",
    "        waiting_cost_i = 1.5*truck_cost if i['inhouse'] == 1000 else 1.5*motobike_cost\n",
    "        T_pi_i = t_oc(i['inhouse']['u_i'], route[0]['v_o'])\n",
    "        initial_waiting_cost = max(route[0]['e_o'] - T_pi_i, 0) * waiting_cost_i\n",
    "        T_pi_i += t_oc(route[0]['v_o'], route[0]['d_o'])\n",
    "        late_delivery_penalty = late_penalty if T_pi_i > route[0]['l_o'] else 0\n",
    "        duplicate_penalty =  0\n",
    "        inhouse_cost = 1\n",
    "        if i['inhouse'] == 1000:\n",
    "            inhouse_cost = truck_cost\n",
    "        else:\n",
    "            inhouse_cost = motobike_cost\n",
    "            \n",
    "        initial_reward = -(inhouse_cost * T_pi_i + initial_waiting_cost + late_delivery_penalty + duplicate_penalty)\n",
    "        # route_rewards.append(initial_reward)\n",
    "        rewards.append(initial_reward)\n",
    "\n",
    "        for n in range(len(route) - 1):\n",
    "            T_pi_i += t_oc(route[n]['d_o'], route[n+1]['v_o'])\n",
    "            duplicate_penalty = duplicate_penalty if i['visited_idx_node'][n+1] in i['visited_idx_node'] else 0\n",
    "            waiting_time_cost = max(route[n+1]['e_o'] - T_pi_i, 0) * waiting_cost_i\n",
    "            T_pi_i += t_oc(route[n+1]['v_o'], route[n+1]['d_o'])\n",
    "            late_delivery_penalty = late_penalty if T_pi_i > route[n+1]['l_o'] else 0\n",
    "\n",
    "            node_reward = -(inhouse_cost * T_pi_i + waiting_time_cost + late_delivery_penalty)\n",
    "            # route_rewards.append(node_reward)\n",
    "            rewards.append(node_reward)\n",
    "\n",
    "        # rewards_per_route_i.append(route_rewards)\n",
    "\n",
    "    # return rewards_per_route_c, rewards_per_route_i \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(num_episodes, \n",
    "                  num_batches=1, \n",
    "                  planning_horizons=10, \n",
    "                  len_periods=10, \n",
    "                  num_trucks=10, \n",
    "                  num_motobikes=20,\n",
    "                  truck_cost=5,\n",
    "                  motobike_cost=3,\n",
    "                  crowdsources_cost=3,\n",
    "                  late_penalty=15,\n",
    "                  duplicate_penalty=10,\n",
    "                  crowdsources_self_cost=1.5\n",
    "                  ):\n",
    "    \n",
    "    policy_network = RoutePolicyNetwork(dim=128, S=1, num_heads=8, num_layers=6)\n",
    "    baseline_network = RoutePolicyNetwork(dim=128, S=1, num_heads=8, num_layers=6)\n",
    "    \n",
    "    pra_network = PRANetwork()\n",
    "    \n",
    "    policy_optimizer = optim.Adam(policy_network.parameters(), lr = 0.095)\n",
    "    pra_optimizer = optim.Adam(pra_network.parameters(), lr = 0.095)\n",
    "    \n",
    "    total_rewards_per_epoch = []\n",
    "    total_losses_per_epoch = []\n",
    "    \n",
    "    for epoch in range(num_episodes):\n",
    "        for batch in range(num_batches):\n",
    "            n_io = np.round(np.random.uniform(90, 120)).astype(int) # Number of initial orders\n",
    "            n_ic = np.round(np.random.uniform(50, 60)).astype(int)  # Number of initial crowdsources\n",
    "        \n",
    "            P_o = []                                                # Orders pool\n",
    "            P_o_c = []                                              # Orders pool for crowdsources from Nash game\n",
    "            P_c = []                                                # Crowdsources pool\n",
    "            T = len_periods                                         # 8:10\n",
    "            P_i = generate_random_inhouse_pool(num_trucks, num_motobikes)              # In-house delivery pool\n",
    "            \n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            pra_probs = []\n",
    "            node_probs = []\n",
    "            baseline_route_rewards = []\n",
    "            for tau in range(planning_horizons):                        # Planning horizons -> #time steps which decision-making process occurs\n",
    "                N_o = np.round(np.random.uniform(30, 45)).astype(int)   # New placed orders\n",
    "                N_c = np.round(np.random.uniform(15, 25)).astype(int)   # New arrived crowdsources\n",
    "                \n",
    "                P_o_tau = []\n",
    "                P_c_tau = []\n",
    "                if tau == 0:\n",
    "                    P_o_tau = generate_random_orders_pool(n_io, 0, len_periods, initial=True) + generate_random_orders_pool(N_o, tau, len_periods)\n",
    "                    P_c_tau = generate_random_crowdsources_pool(n_ic, 0, len_periods, initial=True) + generate_random_crowdsources_pool(N_c, tau, len_periods)\n",
    "                \n",
    "                else:\n",
    "                    P_o_tau = P_o + generate_random_orders_pool(N_o, tau, len_periods)\n",
    "                    P_c_tau = P_c + generate_random_crowdsources_pool(N_c, tau, len_periods)\n",
    "                \n",
    "                nash_game_instance = NashGame(P_o_tau, P_c_tau, crowdsources_cost, motobike_cost, crowdsources_self_cost)\n",
    "                NE_action, NNE_action, game1_result = nash_game_instance.find_NE(P_o_tau, P_c_tau)\n",
    "                # print(list(game1_result.keys()))\n",
    "                \n",
    "                # State S_PRA_tau\n",
    "                n_tau = len(P_o_tau)                                        # Number of current orders\n",
    "                m_tau = len(P_c_tau)                                        # Number of current crowdsources\n",
    "                l_star_o_tau = min(P_o_tau, key=lambda x: x['l_o'])['l_o']  # Late delivery time of the most urgent orders\n",
    "                l_star_c_tau = min(P_c_tau, key=lambda x: x['l_c'])['l_c']  # Last departure time of almost leaving crowdsource\n",
    "                t_ave_tau = t_ave(P_o_tau, P_c_tau)                         # Average travel time between locations of crowdsources and pick-up orders\n",
    "                S_PRA_tau = [n_tau, m_tau, l_star_o_tau, l_star_c_tau, t_ave_tau]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.tensor(S_PRA_tau, dtype=torch.float).unsqueeze(0)\n",
    "                    pra_prob = pra_network(state_tensor)\n",
    "                    action = torch.multinomial(pra_prob, 1) # the decision is selected using the sampling method\n",
    "                    pra_probs.extend(pra_prob)\n",
    "                    \n",
    "                if action.item() == 1:\n",
    "                    routes_policy_c, routes_policy_i, log_probs_tau, node_prob_tau = policy_network(P_o_tau, P_c_tau, P_i, game1_result)   # Lower-agent construct route policy \n",
    "                    route_rewards = calculate_rewards_at_each_node(routes_policy_c, routes_policy_i, t_oc, truck_cost, motobike_cost, late_penalty, duplicate_penalty, crowdsources_cost)\n",
    "                    \n",
    "                    assert len(route_rewards) == len(log_probs_tau), 'Invalid rewards, log_probs'\n",
    "                    \n",
    "                    baseline_routes_policy_c, baseline_routes_policy_i, _ , _ = baseline_network(P_o_tau, P_c_tau, P_i, game1_result)\n",
    "                    baseline_route_reward = calculate_rewards_at_each_node(baseline_routes_policy_c, baseline_routes_policy_i, t_oc, truck_cost, motobike_cost, late_penalty, duplicate_penalty, crowdsources_cost)\n",
    "                    \n",
    "                    \n",
    "                    log_probs.extend(log_probs_tau)\n",
    "                    node_probs.extend(node_prob_tau)\n",
    "                    rewards.extend(route_rewards)\n",
    "                    baseline_route_rewards.extend(baseline_route_reward)\n",
    "                    \n",
    "                    P_o_tau = []\n",
    "                    P_c_tau = []\n",
    "                    \n",
    "                    print(f\"Epoch: {epoch}, Tau: {tau}\")\n",
    "                \n",
    "                P_o.extend(P_o_tau)\n",
    "                T += len_periods                                            # Next length period 8:20, crowdsource 8:15 -> out-dated\n",
    "                P_c_tau = [c for c in P_c_tau if c['l_c'] > T]              # Weed out expired crowd drivers\n",
    "                P_c.extend(P_c_tau)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            policy_rewards = torch.tensor(rewards)\n",
    "            baseline_rewards = torch.tensor(baseline_route_rewards)\n",
    "            \n",
    "            select_node_probs = torch.tensor(node_probs)\n",
    "            \n",
    "            policy_rewards = (policy_rewards - policy_rewards.mean()) / (policy_rewards.std() + 1e-9) # Normalize rewards\n",
    "            baseline_rewards = (baseline_rewards - baseline_rewards.mean()) / (baseline_rewards.std() + 1e-9)\n",
    "            \n",
    "            policy_loss = []\n",
    "            for pra_prob1, node_prob, policy_reward, baseline_reward in zip(pra_probs, select_node_probs, policy_rewards, baseline_rewards):\n",
    "                mean_prob = (pra_prob1 + node_prob).mean()\n",
    "                advantage = policy_reward - baseline_reward\n",
    "                log_sum_prob = torch.log(pra_prob1 + node_prob)\n",
    "                policy_loss.append(mean_prob * advantage * - log_sum_prob)\n",
    "            \n",
    "            # Ensure same tensor size for calculating loss\n",
    "            min_length = min(len(baseline_rewards), len(policy_rewards))\n",
    "            baseline_rewards = baseline_rewards[:min_length]\n",
    "            policy_rewards = policy_rewards[:min_length]\n",
    "            \n",
    "            improve = (baseline_rewards - policy_rewards).mean() >= 0\n",
    "            _, p_value = stats.ttest_rel(baseline_rewards.numpy(), policy_rewards.numpy())  # tensors need to be converted to numpy arrays for stats.ttest_rel\n",
    "            if improve and p_value <= 0.05:\n",
    "                baseline_network.load_state_dict(policy_network.state_dict())\n",
    "        \n",
    "            if len(policy_loss) > 0:\n",
    "                policy_loss = torch.sum(torch.stack(policy_loss))\n",
    "                policy_loss = policy_loss.detach().clone().requires_grad_(True)     #Requires gradient for calculation\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "            pra_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            pra_optimizer.step()\n",
    "            \n",
    "            total_reward = sum(policy_rewards.numpy())\n",
    "            total_rewards_per_epoch.append(total_reward)\n",
    "            \n",
    "            total_loss = policy_loss.item()\n",
    "            total_losses_per_epoch.append(total_loss)\n",
    "            \n",
    "            print(f\"Epoch: {epoch}, Total Reward: {sum(policy_rewards)}, Loss: {policy_loss}\")\n",
    "            \n",
    "    mean_reward = sum(total_rewards_per_epoch) / len(total_rewards_per_epoch)\n",
    "    mean_loss = sum(total_losses_per_epoch) / len(total_losses_per_epoch)\n",
    "\n",
    "    print(f\"Mean Reward: {mean_reward}\")\n",
    "    print(f\"Mean Loss: {mean_loss}\")\n",
    "    \n",
    "    # plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # plt.plot(range(num_episodes), total_rewards_per_epoch, label='Total Rewards', color='blue')\n",
    "    # plt.plot(range(num_episodes), total_losses_per_epoch, label='Losses', color='red')\n",
    "    # plt.legend()\n",
    "    # plt.title('Total Rewards and Losses Over Epochs')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Value')\n",
    "\n",
    "    # plt.show()\n",
    "    \n",
    "    # def plot_routes_with_details(routes_policy_c, routes_policy_i):\n",
    "    #     plt.figure(figsize=(12, 12))\n",
    "    #     colors = plt.cm.rainbow(np.linspace(0, 1, len(routes_policy_c) + len(routes_policy_i)))\n",
    "    #     color_idx = 0\n",
    "\n",
    "    #     # Plot for crowdsourced drivers\n",
    "    #     for driver in routes_policy_c:\n",
    "    #         route = driver['route']\n",
    "    #         if route:\n",
    "    #             color_idx += 1\n",
    "    #             if 'u_c' in driver:\n",
    "    #                 # Plot from initial location u_c to the first v_o\n",
    "    #                 u_c = driver['u_c']\n",
    "    #                 first_vo = route[0]['v_o']\n",
    "    #                 plt.plot([u_c[1], first_vo[1]], [u_c[0], first_vo[0]], color=colors[color_idx], linestyle='--', marker='x')\n",
    "    #             else:\n",
    "    #                 first_vo = route[0]['v_o']\n",
    "\n",
    "    #             for i in range(len(route) - 1):\n",
    "    #                 v_o = route[i]['v_o']\n",
    "    #                 d_o = route[i]['d_o']\n",
    "    #                 plt.plot([v_o[1], d_o[1]], [v_o[0], d_o[0]], color=colors[color_idx], marker='o')\n",
    "    #                 # For the last element, there's no next_vo to plot to, so check if i+1 is within bounds\n",
    "    #                 if i < len(route) - 1:\n",
    "    #                     next_vo = route[i+1]['v_o']\n",
    "    #                     # Plot d_o to next v_o\n",
    "    #                     plt.plot([d_o[1], next_vo[1]], [d_o[0], next_vo[0]], color=colors[color_idx], linestyle='--')\n",
    "    #         color_idx += 1\n",
    "\n",
    "    #     # Plot for in-house drivers\n",
    "    #     for driver in routes_policy_i:\n",
    "    #         route = driver['route']\n",
    "    #         if route:\n",
    "    #             for i in range(len(route) - 1):\n",
    "    #                 v_o = route[i]['v_o']\n",
    "    #                 d_o = route[i]['d_o']\n",
    "    #                 # Plot v_o to d_o\n",
    "    #                 plt.plot([v_o[1], d_o[1]], [v_o[0], d_o[0]], color=colors[color_idx], marker='o')\n",
    "    #                 if i < len(route) - 1:\n",
    "    #                     next_vo = route[i+1]['v_o']\n",
    "    #                     # Plot d_o to next v_o\n",
    "    #                     plt.plot([d_o[1], next_vo[1]], [d_o[0], next_vo[0]], color=colors[color_idx], linestyle='--')\n",
    "    #         color_idx += 1\n",
    "\n",
    "    #     plt.xlabel('Longitude')\n",
    "    #     plt.ylabel('Latitude')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "    \n",
    "    calculate_cost(routes_policy_c, routes_policy_i, t_oc, truck_cost, motobike_cost, late_penalty, duplicate_penalty, crowdsources_cost)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Tau: 2\n",
      "Epoch: 0, Tau: 3\n"
     ]
    }
   ],
   "source": [
    "training_loop(1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
